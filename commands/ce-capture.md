# Capture Session Knowledge

> Claude slash command for in-session chatlog capture

<!-- Generated by slash_command_generator.py from specs/modules/runtime-command-chatlog-capture.yaml -->
<!-- Do not edit manually - regenerate using: python3 scripts/generate.py -->


## Overview

Extract architectural decisions, constraints, and invariants from this session into a structured chatlog.

**Output**: YAML chatlog file for database ingestion

---

## Step 1: Review Project Vocabulary

### ⚠️ REQUIRED: Allowed Domains

**You MUST use ONLY these domain values when creating rules.**

- `__DOMAIN_1__`
- `__DOMAIN_2__`
- `__DOMAIN_3__`
- `__DOMAIN_4__`
- `__DOMAIN_5__`
- `__DOMAIN_6__`
- `__DOMAIN_7__`
- `__DOMAIN_8__`

**Forbidden domains:**
Do NOT use generic terms like:
- ❌ `architecture` (use `system` or specific domain instead)
- ❌ `methodology` (use `system` instead)
- ❌ `tooling` (use `system` instead)
- ❌ `governance` (use `validation` or `system` instead)
- ❌ `testing` (use `validation` instead)
- ❌ Any other domain not listed above

**Why this matters:** Validation will FAIL if you use domains not in this list. Save time by using correct domains from the start.

### ⚠️ REQUIRED: Tag Naming Convention

**You MUST use kebab-case for all tags.**

✅ **Correct format:**
- `invoice-handling`
- `xml-validation`
- `error-recovery`
- `data-transform-v2` (numbers allowed)

❌ **Incorrect format:**
- `invoice_handling` (snake_case - use hyphens, not underscores)
- `invoiceHandling` (camelCase - use lowercase + hyphens)
- `Invoice-Handling` (PascalCase - use lowercase only)
- `invoice handling` (spaces not allowed)

**Why:** Tags must be kebab-case for:
- Consistency across the vocabulary
- URL-safe encoding
- Grep-ability and text processing

**Validation will REJECT tags that don't match pattern:** `^[a-z0-9]+(-[a-z0-9]+)*$`

---

## Step 2: Extract Session Knowledge

### Context Engine Self-Reference Filter (IMPORTANT)

**Before extracting each rule**, evaluate: Is this rule about the **project** or about **Context Engine itself**?

**EXCLUDE rules about**:
- Context Engine infrastructure (chatlog capture, extraction, onboarding workflows)
- Debugging CE tooling (validate_chatlog.py errors, schema version issues)
- CE-specific constraints (CAP-*, EXT-*, ONB-*, RREL-*, INIT-*, VOC-*, SCH-*, TMP-*)
- CE commands (/ce-capture, /ce-init, /onboard-generate)
- CE file formats (chatlog YAML schema, onboard-root.yaml structure)
- CE configuration (deployment.yaml, tag-vocabulary.yaml)
- CE database schema (rules.db table structure, metadata columns)

**INCLUDE rules about**:
- Your project's architecture and domain logic
- Business requirements and constraints
- Technical decisions for your application
- Implementation patterns specific to your project

**CE Keywords** (if rule title/description contains these, likely CE-internal):
`context engine, chatlog, ce-init, ce-capture, ce-onboard, validate_chatlog.py, extract.py,
onboard.py, rules.db, deployment.yaml, tag-vocabulary.yaml, onboard-root.yaml,
schema_version, metadata column, chatlog_id, reusability_scope, tags_state,
tier_1_domains, tier_2_tags, CAP-*, EXT-*, ONB-*, RREL-*, INIT-*, VOC-*, SCH-*,
TMP-*, KT-*, QC-*, CS-*, OPT-*`

**Context-aware evaluation**: If debugging a project feature that happens to use SQLite,
that's project context (INCLUDE). If debugging Context Engine's rules.db schema,
that's CE-internal (EXCLUDE).

**At end of extraction**: Report count of skipped CE-internal rules:
"Note: Skipped [N] rules about Context Engine itself (infrastructure, not project architecture)"

---

### Rule Type Selection Guide

Use this decision tree to choose the correct rule type:

**For ADRs (Architectural Decision Records):**
- Did we choose between multiple alternatives?
- Can you list the rejected options?
- Does it involve tradeoffs?
→ If yes: **ADR**

**For CONs (Constraints):**
- Can this be tested mechanically (grep, SQL, pytest)?
- Does it constrain WHAT the artifact must be (not WHEN)?
- Is it a single, testable behavior?
→ If yes: **CON**

**For INVs (Invariants):**
- Is this always true regardless of context?
- Would violating this break the system?
- Is it a foundational assumption?
→ If yes: **INV**

**For PATs (Patterns):**
- Is this an established convention or design pattern?
- Does it describe a coding standard or anti-pattern to avoid?
- Is it a reusable solution pattern?
→ If yes: **PAT**

---

### Quality Checklist

Before proceeding to validation, verify:

**✓ Single Behavior**
- Each CON describes ONE testable behavior
- No "and also", "in addition", or multiple semicolons
- If complex, split into multiple separate CONs

**✓ No Temporal Language**
- Avoid: "was", "during Phase N", "after X completed"
- Rules describe current/future state, not history
- Historical context goes in metadata, not rationale

**✓ Artifact vs Process**
- CONs describe WHAT code/data must be
- Not WHEN to design or HOW to implement
- Focus on properties, not timeline

**✓ Tag Consistency**
- Rules referencing same decision use similar tags
- Related rules are discoverable together

**✓ Testability for CONs**
- Every CON has a `validation_method`
- Examples:
  - `grep -r "pattern" src/` (code pattern)
  - `SELECT COUNT(*) FROM table WHERE condition` (data constraint)
  - `pytest tests/test_feature.py` (behavior test)
  - `Code review: Check X in file Y` (manual validation)

---

### Relationship Tracking (Optional)

If this session builds upon or relates to rules from previous sessions, you can track these relationships:

**Four Relationship Types:**

1. **implements** - This rule enforces/realizes an architectural decision
   - Example: CON-00456 implements ADR-00123 (Constraint enforces architecture)

2. **extends** - This rule adds specificity to another rule
   - Example: CON-00457 extends CON-00456 (Adds detail to existing constraint)

3. **conflicts_with** - This rule contradicts another rule (needs resolution)
   - Example: INV-00234 conflicts_with INV-00189 (Flag for discussion)

4. **related_to** - Thematic connection without hierarchy
   - Example: PAT-00567 related_to PAT-00543 (Similar concepts, no dependency)

**When to Use:**
- Only if you remember specific rule IDs from prior sessions
- Only if the relationship is meaningful (not just "both about same topic")
- Relationships are OPTIONAL - omit the field entirely if no clear relationships

**Format:**
```yaml
relationships:
  - type: "implements"
    target: "ADR-00123"
    rationale: "Enforces three-layer parsing order from architecture decision"
```

---

### Code Linking (Optional)

If this rule describes or governs existing code, you can link to implementation files:

**Three Reference Types:**

1. **implements** - Code file that realizes this rule's requirements
   - Example: `src/parser/tax_hierarchy.py` implements ADR-00123

2. **validates** - Test file that validates this rule's correctness
   - Example: `tests/unit/test_tax.py` validates CON-00456

3. **documents** - Documentation that explains this rule's context
   - Example: `docs/architecture/tax-design.md` documents ADR-00123

**When to Use:**
- Only if code implementing the rule already exists
- Only if you know the specific file location
- Line ranges are optional but helpful (e.g., [45, 120] for lines 45-120)
- implementation_refs is OPTIONAL - omit entirely if no code links

**Format:**
```yaml
implementation_refs:
  - type: "implements"
    file: "src/parser/tax_hierarchy.py"
    lines: [45, 120]  # Optional
    role_description: "Three-layer parsing logic"
```

**Benefits:**
- Connects abstract rules to concrete code
- Enables agents to validate understanding by reading implementation
- Transforms passive onboarding into actionable guidance

---

## Step 3: Locate Context Engine and Create Chatlog

### Find Context Engine Installation

First, locate the Context Engine installation:

```bash
# Find .context-engine home from deployment config
CONTEXT_ENGINE_HOME=$(python3 -c "
import yaml
from pathlib import Path

# Find deployment.yaml in common locations
config_locations = [
    Path.cwd() / '.context-engine' / 'config' / 'deployment.yaml',
    Path.home() / '.context-engine' / 'config' / 'deployment.yaml',
]

for config_path in config_locations:
    if config_path.exists():
        with open(config_path) as f:
            config = yaml.safe_load(f)
        print(config['paths']['context_engine_home'])
        break
" 2>/dev/null || echo ".context-engine")

echo "Context Engine: $CONTEXT_ENGINE_HOME"

# Ensure chatlogs directory exists
mkdir -p "$CONTEXT_ENGINE_HOME/data/chatlogs"
```

### Create Chatlog File

**IMPORTANT:** Create the chatlog file directly in `$CONTEXT_ENGINE_HOME/data/chatlogs/` directory.

**Filename Format:** `YYYYmmdd-HHMMSSZ_uuidprefix.yaml`
- Example: `20251021-212312Z_676b26a2.yaml`
- Timestamp in UTC (YYYYmmdd-HHMMSSZ format)
- First 8 characters of UUID
- Enables chronological sorting and uniqueness

**Location:** `$CONTEXT_ENGINE_HOME/data/chatlogs/<filename>.yaml`

**Domain Assignment Rules:**
Before filling in the template, remember:
1. **Use ONLY domains from Step 1** (the exact domain names listed there)
2. **Do NOT use domains from general knowledge** (architecture, methodology, tooling, governance, testing, etc.)
3. Each rule needs exactly one domain
4. Domain values are case-sensitive (use lowercase)

Generate the chatlog using this template:

```yaml
# Obtain chatlog_id and timestamp from system
# chatlog_id: $(uuidgen)  # Run: uuidgen
# timestamp: $(date -u +"%Y-%m-%dT%H:%M:%SZ")  # Run: date -u +"%Y-%m-%dT%H:%M:%SZ"
#
# Filename format: YYYYmmdd-HHMMSSZ_uuidprefix.yaml
# - Convert timestamp 2025-10-21T21:23:12Z → 20251021-212312Z
# - Take first 8 chars of UUID 676b26a2-f0fe-... → 676b26a2
# - Result: 20251021-212312Z_676b26a2.yaml

chatlog_id: "<INSERT_UUID_HERE>"
schema_version: "v1.13.0"
timestamp: "<INSERT_TIMESTAMP_HERE>"
agent: "Claude (Sonnet 4.5)"  # Or appropriate model
session_duration_minutes: <ESTIMATE_DURATION>

rules:
  decisions:
    - topic: "Descriptive statement of decision"
      rationale: "Why this decision was made, context and reasoning"
      domain: "<STEP_1_DOMAIN>"  # MUST be from Step 1's Allowed Domains list
      confidence: 0.9  # 0.0-1.0
      alternatives_rejected:
        - "Alternative 1 we considered but rejected"
        - "Alternative 2 we considered but rejected"
      context_when_applies: "When this decision is relevant"
      context_when_not: "When this decision doesn't apply"
      tradeoffs: "What we sacrificed for this decision"
      relationships:  # OPTIONAL - Omit if no relationships
        - type: "extends"  # or: implements, conflicts_with, related_to
          target: "ADR-00123"  # Rule ID from previous session
          rationale: "Brief explanation of relationship"
      implementation_refs:  # OPTIONAL - Omit if no code links
        - type: "implements"  # or: validates, documents
          file: "src/parser/tax_hierarchy.py"
          lines: [45, 120]  # Optional line range
          role_description: "Three-layer tax parsing logic"

  constraints:
    - topic: "Single testable constraint statement"
      rationale: "Why this constraint exists"
      domain: "<STEP_1_DOMAIN>"  # MUST be from Step 1's Allowed Domains list
      confidence: 0.85
      validation_method: "How to verify (grep, SQL, pytest, code review)"
      relationships:  # OPTIONAL - Omit if no relationships
        - type: "implements"  # or: extends, conflicts_with, related_to
          target: "ADR-00123"  # Rule ID this constraint enforces
          rationale: "Brief explanation of relationship"
      implementation_refs:  # OPTIONAL - Omit if no code links
        - type: "validates"  # or: implements, documents
          file: "tests/unit/test_tax_parsing.py"
          lines: [89, 145]  # Optional line range
          role_description: "Validates layer dependency order"

  invariants:
    - topic: "Always-true system property"
      rationale: "Why this is always true"
      domain: "<STEP_1_DOMAIN>"  # MUST be from Step 1's Allowed Domains list
      confidence: 0.95
      relationships:  # OPTIONAL - Omit if no relationships
        - type: "related_to"  # or: implements, extends, conflicts_with
          target: "INV-00042"  # Rule ID of related invariant
          rationale: "Brief explanation of relationship"
      implementation_refs:  # OPTIONAL - Omit if no code links
        - type: "documents"  # or: implements, validates
          file: "docs/architecture/tax-design.md"
          role_description: "Tax architecture overview"

session_context:
  problem_solved: "High-level summary of what was accomplished"
  patterns_applied:
    - "Reusable pattern 1"
    - "Reusable pattern 2"
  anti_patterns_avoided:
    - "Bad pattern avoided with reasoning"
  conventions_established:
    - "Naming convention or standard adopted"
  reusability_scope:
    project_wide:
      - "<Rule indices applicable to entire project>"
    module_scoped:
      <module_name>:
        - "<Rule indices scoped to specific module>"
    historical:
      - "<Rule indices providing context only, not active guidance>"

artifacts:
  files_modified:
    - "path/to/modified/file.py"
  commands_executed:
    - "command that was run"
```

---

## Step 4: Pre-Validation Self-Check

Before running validation, verify domains are correct:

```bash
# Set the chatlog filename (use the file you created in Step 3)
CHATLOG_FILE="$CONTEXT_ENGINE_HOME/data/chatlogs/<your-chatlog-file>.yaml"

# Quick check: Are all domain values valid?
grep "^  domain:" "$CHATLOG_FILE" | sort | uniq
```

**Expected output:** Only domains from Step 1's Allowed Domains list

**If you see any of these, FIX BEFORE VALIDATION:**
- ❌ `architecture`, `methodology`, `tooling`, `governance`, `testing`
- ❌ Any domain not listed in Step 1

This quick check saves time by catching domain errors before validation.

---

## Step 5: Validate and Remediate

**Validation with automatic remediation for common errors:**

This step uses a deterministic remediation loop that automatically fixes the 5 most common validation errors (covers ~80% of issues).

**Note:** The chatlog file is already in its final location (`$CONTEXT_ENGINE_HOME/data/chatlogs/`), so validation and remediation happen in-place.

### Validation Loop (3 attempts with automatic fixes)

```bash
# Use the CHATLOG_FILE variable from Step 4
# Format: $CONTEXT_ENGINE_HOME/data/chatlogs/<your-chatlog-file>.yaml

for attempt in {{1..3}}; do
  echo "=== Validation attempt $attempt/3 ==="

  # Run validator
  RESULT=$(python3 "${{CONTEXT_ENGINE_HOME}}/scripts/validate_chatlog.py" "$CHATLOG_FILE")

  # Check if valid
  if echo "$RESULT" | python3 -c "import sys, json; result = json.load(sys.stdin); sys.exit(0 if result.get('valid') else 1)" 2>/dev/null; then
    echo "✓ Validation passed!"
    break
  fi

  # Show errors
  echo "Validation errors found:"
  echo "$RESULT" | python3 -c "import sys, json; result = json.load(sys.stdin); [print(f\"  - {{e['message']}}\") for e in result.get('errors', [])]"

  if [ $attempt -eq 3 ]; then
    echo ""
    echo "⚠ Maximum remediation attempts reached"
    echo "Saving as ${{CHATLOG_FILE}}.invalid for manual review"
    mv "$CHATLOG_FILE" "${{CHATLOG_FILE}}.invalid"
    echo "$RESULT" > "${{CHATLOG_FILE}}.errors.json"
    echo ""
    echo "Next steps:"
    echo "1. Review ${{CHATLOG_FILE}}.errors.json"
    echo "2. Manually fix ${{CHATLOG_FILE}}.invalid"
    echo "3. Rename back to .yaml and re-validate"
    exit 1
  fi

  echo ""
  echo "Applying automatic fixes (attempt $attempt)..."

  # Pattern 1: Invalid domain → fuzzy match to allowed domains
  if echo "$RESULT" | grep -q "domain.*not in allowed list"; then
    echo "  → Fixing invalid domains with fuzzy matching..."
    python3 << 'EOF'
import sys, json, yaml
from difflib import get_close_matches
from pathlib import Path

# Load error result
with open('${{CHATLOG_FILE}}') as f:
    chatlog = yaml.safe_load(f)

# Load vocabulary for valid domains
vocab_path = Path('${{CONTEXT_ENGINE_HOME}}') / 'config' / 'tag-vocabulary.yaml'
with open(vocab_path) as f:
    vocab = yaml.safe_load(f)
valid_domains = [d.replace('-', '_') for d in vocab.get('tier_1_domains', [])]

# Fix invalid domains in all rule categories
fixed_count = 0
for category in ['decisions', 'constraints', 'invariants']:
    for rule in chatlog.get('rules', {{}}).get(category, []):
        domain = rule.get('domain', '')
        if domain not in valid_domains:
            # Fuzzy match (cutoff=0.6 is forgiving but not too loose)
            matches = get_close_matches(domain, valid_domains, n=1, cutoff=0.6)
            new_domain = matches[0] if matches else valid_domains[0]
            rule['domain'] = new_domain
            print("    '" + domain + "' → '" + new_domain + "'")
            fixed_count += 1

if fixed_count > 0:
    with open('${{CHATLOG_FILE}}', 'w') as f:
        yaml.dump(chatlog, f, default_flow_style=False, sort_keys=False)
    print("  ✓ Fixed " + str(fixed_count) + " domain(s)")
EOF
  fi

  # Pattern 2: Confidence out of range → clamp to valid range
  if echo "$RESULT" | grep -q "confidence.*out of range"; then
    echo "  → Clamping confidence values to [0.0, 1.0]..."
    # Values > 1.0 → 0.95 (high confidence)
    sed -i.bak 's/confidence: [1-9][0-9]*\.[0-9]*/confidence: 0.95/g' "$CHATLOG_FILE"
    sed -i.bak 's/confidence: [1-9][0-9]*$/confidence: 0.95/g' "$CHATLOG_FILE"
    # Values < 0.0 → 0.5 (medium confidence)
    sed -i.bak 's/confidence: -[0-9.]*$/confidence: 0.5/g' "$CHATLOG_FILE"
    rm -f "${{CHATLOG_FILE}}.bak"
    echo "  ✓ Confidence values clamped"
  fi

  # Pattern 3: Invalid UUID → regenerate
  if echo "$RESULT" | grep -q "chatlog_id.*not.*valid UUID"; then
    echo "  → Regenerating chatlog_id..."
    NEW_UUID=$(python3 -c "import uuid; print(uuid.uuid4())")
    sed -i.bak "s/chatlog_id: .*/chatlog_id: \"$NEW_UUID\"/g" "$CHATLOG_FILE"
    rm -f "${{CHATLOG_FILE}}.bak"
    echo "  ✓ New UUID: $NEW_UUID"
  fi

  # Pattern 4: Invalid timestamp → regenerate
  if echo "$RESULT" | grep -q "timestamp.*not.*ISO 8601\|timestamp.*missing.*UTC"; then
    echo "  → Regenerating timestamp..."
    NEW_TS=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
    sed -i.bak "s/timestamp: .*/timestamp: \"$NEW_TS\"/g" "$CHATLOG_FILE"
    rm -f "${{CHATLOG_FILE}}.bak"
    echo "  ✓ New timestamp: $NEW_TS"
  fi

  # Pattern 5: Missing validation_method in constraints → add default
  if echo "$RESULT" | grep -q "missing.*validation_method"; then
    echo "  → Adding default validation_method to constraints..."
    python3 << 'EOF'
import yaml

with open('${{CHATLOG_FILE}}') as f:
    chatlog = yaml.safe_load(f)

fixed_count = 0
for constraint in chatlog.get('rules', {{}}).get('constraints', []):
    if 'validation_method' not in constraint:
        constraint['validation_method'] = "Code review required"
        fixed_count += 1

if fixed_count > 0:
    with open('${{CHATLOG_FILE}}', 'w') as f:
        yaml.dump(chatlog, f, default_flow_style=False, sort_keys=False)
    print("  ✓ Added validation_method to " + str(fixed_count) + " constraint(s)")
EOF
  fi

  echo ""
done

echo "✓ Chatlog validated successfully: $CHATLOG_FILE"
```

### What This Does

The validation loop handles these common errors automatically:

1. **Invalid domains** → Fuzzy match to closest allowed domain (e.g., 'architect' → 'architecture')
2. **Confidence out of range** → Clamp to [0.0, 1.0] (>1.0 → 0.95, <0.0 → 0.5)
3. **Invalid UUID** → Generate new UUID v4
4. **Invalid timestamp** → Generate new ISO 8601 UTC timestamp
5. **Missing validation_method** → Add default "Code review required"

**If validation still fails after 3 attempts:**
- File saved as `.invalid` for manual review
- Errors saved to `.errors.json` for debugging
- This catches edge cases not covered by the 5 patterns (~20% of errors)

**If validation succeeds:**
- Proceed to Step 5.1 for warning triage

---

## Step 5.1: Triage Warnings (If Any)

**✓ Validation passed with warnings. Chatlog is valid and ready to commit.**

Warnings are **quality suggestions** based on pattern matching, not blocking errors. You must review and decide whether to fix or accept each warning.

### Understanding Warning Severity

Severity levels indicate **pattern matching confidence**, not absolute truth:

| Severity | Confidence | Meaning | Recommended Action |
|----------|------------|---------|-------------------|
| **HIGH** | >80% | Strong signal - likely multi-behavior or lifecycle issue | Review carefully, fix if accurate |
| **MEDIUM** | 50-80% | Moderate signal - possible issue | Quick assessment recommended |
| **LOW** | <50% | Weak signal - high false positive rate | Accept unless obviously wrong |

### Decision Tree: Fix vs. Accept

```
IF severity = HIGH:
  → Read warning message
  → Assess if accurate (does rule actually have multiple behaviors?)
  → If yes: Fix (split constraint, rewrite rationale, etc.)
  → If no: Accept with mental note

ELSE IF severity = MEDIUM:
  → Quick review (does warning make sense?)
  → If obviously wrong: Fix
  → Else: Accept

ELSE IF severity = LOW:
  → Accept (unless warning reveals actual error you missed)
```

### Examples of Warnings

**✓ ACCEPTABLE (Low severity - legitimate construct):**

```
Warning: [LOW] Possible multi-behavior CON (contains ' and ')
Text: "Logging MUST capture problem description, resolution options, and business impact"
Reasoning: Single cohesive behavior - all three elements are part of one logging requirement
Verdict: ACCEPT
```

**✗ FIX REQUIRED (High severity - actual multi-behavior):**

```
Warning: [HIGH] Possible multi-behavior CON (contains 'and also')
Text: "Validator checks schema compliance and also optimizes performance"
Reasoning: Two unrelated behaviors - split into separate constraints per INV-002
Verdict: FIX (create two constraints)
```

**? CONTEXT-DEPENDENT (Medium severity - assess intent):**

```
Warning: [MEDIUM] Temporal language detected (contains 'was')
Text: "Decision rationale explains why alternative X was rejected"
Reasoning: 'was rejected' describes decision context, not lifecycle state
Verdict: ACCEPT (legitimate use of past tense in rationale)
```

### Remember: Pattern Matching Has Limitations

Validators use **pattern matching, not semantic analysis**. LOW severity warnings often flag legitimate constructs. For example:

- `' and '` appears in 40%+ of constraints, but only ~10% are actually multi-behavior
- Compound requirements like "capture X, Y, and Z" are often cohesive single behaviors
- Temporal language in rationales is legitimate when explaining decision context

**Trust your semantic judgment over pattern heuristics.**

### If You Decide to Fix Warnings

Use the Edit tool to update the chatlog file, then re-validate:

```bash
# After fixing warnings with Edit tool
python3 "${{CONTEXT_ENGINE_HOME}}/scripts/validate_chatlog.py" "$CHATLOG_FILE"
```

Repeat until warnings are resolved or you've accepted remaining warnings as false positives.

---

## Step 6: Save and Commit

Once validated:

```bash
# Chatlog is already in the correct location: $CONTEXT_ENGINE_HOME/data/chatlogs/

# Commit with provenance
git add "$CHATLOG_FILE"
git commit -m "Add session chatlog: <short-description>

Co-Authored-By: Claude <noreply@anthropic.com>"
```

✓ Session knowledge captured!

---

## Next Steps

To process this chatlog and extract rules to the database:

**Option 1: Slash Command (Interactive)**
```
/ce-extract
```
Uses Claude Code to run extraction with status updates and error handling.

**Option 2: Make Command (CLI/Automation)**
```bash
# Navigate to Context Engine directory
cd "${{CONTEXT_ENGINE_HOME}}"

# Extract chatlogs to database
make chatlogs-extract

# OR run the full CI pipeline (extract + optimize-tags + validation)
make ci-pipeline
```

**Option 3: Direct Script (Advanced)**
```bash
# If Make unavailable
python3 scripts/extract.py
```

**Recommendation**: Use `/ce-extract` for interactive sessions or `make chatlogs-extract` for automation/CI. Both provide the same functionality with different interfaces.
